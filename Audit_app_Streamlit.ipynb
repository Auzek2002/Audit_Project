{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYyVxbo5Bi3J",
        "outputId": "35fa4607-f243-4ca6-dc8b-99673702cdfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "os.environ[\"STREAMLIT_WATCHER_TYPE\"] = \"none\"\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import outlines\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Imports\n",
        "from enum import Enum\n",
        "from typing import Literal, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "# For pretty printing\n",
        "from rich import print\n",
        "from rich.panel import Panel\n",
        "from rich.table import Table\n",
        "from rich.console import Console\n",
        "from rich.text import Text\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "# Severity levels are used classify the severity of a security event.\n",
        "# High severity events are those that should be escalated to a human\n",
        "# for further investigation.\n",
        "class SeverityLevel(str, Enum):\n",
        "    CRITICAL = \"CRITICAL\"\n",
        "    HIGH = \"HIGH\"\n",
        "    MEDIUM = \"MEDIUM\"\n",
        "    LOW = \"LOW\"\n",
        "    INFO = \"INFO\"\n",
        "\n",
        "# Attack types are used to classify security events. This is not an exhaustive\n",
        "# list of attack vectors!\n",
        "class AttackType(str, Enum):\n",
        "    BRUTE_FORCE = \"BRUTE_FORCE\"\n",
        "    SQL_INJECTION = \"SQL_INJECTION\"\n",
        "    XSS = \"XSS\"\n",
        "    FILE_INCLUSION = \"FILE_INCLUSION\"\n",
        "    COMMAND_INJECTION = \"COMMAND_INJECTION\"\n",
        "    PRIVILEGE_ESCALATION = \"PRIVILEGE_ESCALATION\"\n",
        "    UNKNOWN = \"UNKNOWN\"\n",
        "\n",
        "# A WebTrafficPattern is a pattern of traffic to a web server --\n",
        "# it highlights commonly accessed URLs, methods, and response codes.\n",
        "#\n",
        "# WebTrafficPatterns are low-priority summarizations used to help\n",
        "# with understanding the overall traffic patterns to a web server.\n",
        "class WebTrafficPattern(BaseModel):\n",
        "    url_path: str\n",
        "    http_method: str\n",
        "    hits_count: int\n",
        "    response_codes: dict[str, int]  # Maps status code to count\n",
        "    unique_ips: int\n",
        "# A LogID is a unique identifier for a log entry. The code in this\n",
        "# script injects a LOGID-<LETTERS> identifier at the beginning of\n",
        "# each log entry, which we can use to identify the log entry.\n",
        "# Language models are fuzzy and they often cannot completely\n",
        "# copy the original log entry verbatim, so we use the LOGID\n",
        "# to retrieve the original log entry.\n",
        "class LogID(BaseModel):\n",
        "    log_id: str = Field(\n",
        "        description=\"\"\"\n",
        "        The ID of the log entry in the format of LOGID-<LETTERS> where\n",
        "        <LETTERS> indicates the log identifier at the beginning of\n",
        "        each log entry.\n",
        "        \"\"\",\n",
        "\n",
        "        # This is a regular expression that matches the LOGID-<LETTERS> format.\n",
        "        # The model will fill in the <LETTERS> part.\n",
        "        pattern=r\"LOGID-([A-Z]+)\",\n",
        "    )\n",
        "\n",
        "    # Find the log entry in a list of logs. Simple\n",
        "    # conveience function.\n",
        "    def find_in(self, logs: list[str]) -> Optional[str]:\n",
        "        for log in logs:\n",
        "            if self.log_id in log:\n",
        "                return log\n",
        "        return None\n",
        "# Class for an IP address.\n",
        "class IPAddress(BaseModel):\n",
        "    ip_address: str = Field(\n",
        "        pattern=r\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\",\n",
        "    )\n",
        "\n",
        "# Class for an HTTP response code.\n",
        "class ResponseCode(BaseModel):\n",
        "    response_code: str = Field(\n",
        "        pattern=r\"^\\d{3}$\",\n",
        "    )\n",
        "# A WebSecurityEvent is a security event that occurred on a web server.\n",
        "#\n",
        "# WebSecurityEvents are high-priority events that should be escalated\n",
        "# to a human for further investigation.\n",
        "class WebSecurityEvent(BaseModel):\n",
        "    # The log entry IDs that are relevant to this event.\n",
        "    relevant_log_entry_ids: list[LogID]\n",
        "\n",
        "    # The reasoning for why this event is relevant.\n",
        "    reasoning: str\n",
        "\n",
        "    # The type of event.\n",
        "    event_type: str\n",
        "\n",
        "    # The severity of the event.\n",
        "    severity: SeverityLevel\n",
        "\n",
        "    # Whether this event requires human review.\n",
        "    requires_human_review: bool\n",
        "\n",
        "    # The confidence score for this event. I'm not sure if this\n",
        "    # is meaningful for language models, but it's here if we want it.\n",
        "    confidence_score: float = Field(\n",
        "        ge=0.0,\n",
        "        le=1.0,\n",
        "        description=\"Confidence score between 0 and 1\"\n",
        "    )\n",
        "\n",
        "    # Web-specific fields\n",
        "    url_pattern: str = Field(\n",
        "        min_length=1,\n",
        "        description=\"URL pattern that triggered the event\"\n",
        "    )\n",
        "\n",
        "    http_method: Literal[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\", \"HEAD\", \"TRACE\", \"CONNECT\"]\n",
        "    source_ips: list[IPAddress]\n",
        "    response_codes: list[ResponseCode]\n",
        "    user_agents: list[str]\n",
        "\n",
        "    # Possible attack patterns for this event.\n",
        "    possible_attack_patterns: list[AttackType]\n",
        "\n",
        "    # Recommended actions for this event.\n",
        "    recommended_actions: list[str]\n",
        "# A LogAnalysis is a high-level analysis of a set of logs.\n",
        "class LogAnalysis(BaseModel):\n",
        "    # A summary of the analysis.\n",
        "    summary: str\n",
        "\n",
        "    # Observations about the logs.\n",
        "    observations: list[str]\n",
        "\n",
        "    # Planning for the analysis.\n",
        "    planning: list[str]\n",
        "\n",
        "    # Security events found in the logs.\n",
        "    events: list[WebSecurityEvent]\n",
        "\n",
        "    # Traffic patterns found in the logs.\n",
        "    traffic_patterns: list[WebTrafficPattern]\n",
        "\n",
        "    # The highest severity event found.\n",
        "    highest_severity: Optional[SeverityLevel]\n",
        "    requires_immediate_attention: bool\n",
        "def format_log_analysis(analysis: LogAnalysis, logs: list[str]):\n",
        "    \"\"\"Format a LogAnalysis object into a rich console output.\n",
        "\n",
        "    Args:\n",
        "        analysis: A LogAnalysis object (not a list)\n",
        "        logs: List of original log entries with LOGID prefixes\n",
        "    \"\"\"\n",
        "    console = Console()\n",
        "\n",
        "    # Create header\n",
        "    header = Panel(\n",
        "        f\"[bold yellow]Log Analysis Report[/]\\n[blue]{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}[/]\",\n",
        "        border_style=\"yellow\"\n",
        "    )\n",
        "\n",
        "    # Create observations section\n",
        "    observations = Table(show_header=True, header_style=\"bold magenta\", show_lines=True)\n",
        "    observations.add_column(\"Key Observations\", style=\"cyan\")\n",
        "    for obs in analysis.observations:\n",
        "        observations.add_row(obs)\n",
        "\n",
        "    # Create security events section\n",
        "    events_table = Table(show_header=True, header_style=\"bold red\", show_lines=True)\n",
        "    events_table.add_column(\"Security Events\", style=\"red\")\n",
        "    events_table.add_column(\"Details\", style=\"yellow\")\n",
        "\n",
        "    # Create a log table if there are any relevant log entry IDs\n",
        "    event_logs_table = Table(show_header=True, header_style=\"bold cyan\", show_lines=True)\n",
        "    event_logs_table.add_column(\"Related Log Entries\", style=\"cyan\", width=100)\n",
        "\n",
        "    for event in analysis.events:\n",
        "        event_details = [\n",
        "            f\"Type: {event.event_type}\",\n",
        "            f\"Severity: {event.severity.value}\",\n",
        "            f\"Confidence: {event.confidence_score * 100}%\",\n",
        "            f\"Source IPs: {', '.join([ip.ip_address for ip in event.source_ips])}\",\n",
        "            f\"URL Pattern: {event.url_pattern}\",\n",
        "            f\"Possible Attacks: {', '.join([attack.value for attack in event.possible_attack_patterns])}\"\n",
        "        ]\n",
        "        events_table.add_row(\n",
        "            Text(event.event_type, style=\"bold red\"),\n",
        "            \"\\n\".join(event_details)\n",
        "        )\n",
        "\n",
        "        # Add related logs to the table\n",
        "        for log_id in event.relevant_log_entry_ids:\n",
        "            log = log_id.find_in(logs)\n",
        "            if log:\n",
        "                event_logs_table.add_row(log)\n",
        "\n",
        "    # Create traffic patterns section\n",
        "    traffic_table = Table(show_header=True, header_style=\"bold green\", show_lines=True)\n",
        "    traffic_table.add_column(\"URL Path\", style=\"green\")\n",
        "    traffic_table.add_column(\"Method\", style=\"cyan\")\n",
        "    traffic_table.add_column(\"Hits\", style=\"yellow\")\n",
        "    traffic_table.add_column(\"Status Codes\", style=\"magenta\")\n",
        "\n",
        "    for pattern in analysis.traffic_patterns:\n",
        "        traffic_table.add_row(\n",
        "            pattern.url_path,\n",
        "            pattern.http_method,\n",
        "            str(pattern.hits_count),\n",
        "            \", \".join(f\"{k}: {v}\" for k, v in pattern.response_codes.items()),\n",
        "        )\n",
        "\n",
        "    # Create summary panel\n",
        "    summary_text = f\"[bold white]Summary:[/]\\n[cyan]{analysis.summary}[/]\\n\\n\"\n",
        "    if analysis.highest_severity:\n",
        "        summary_text += f\"[bold red]Highest Severity: {analysis.highest_severity.value}[/]\\n\"\n",
        "    summary_text += f\"[bold {'red' if analysis.requires_immediate_attention else 'green'}]\" + \\\n",
        "                   f\"Requires Immediate Attention: {analysis.requires_immediate_attention}[/]\"\n",
        "\n",
        "    summary = Panel(\n",
        "        summary_text,\n",
        "        border_style=\"blue\"\n",
        "    )\n",
        "\n",
        "    # Print everything\n",
        "    console.print(header)\n",
        "    console.print(\"\\n[bold blue]📝 Analysis Summary:[/]\")\n",
        "    console.print(summary)\n",
        "    console.print(observations)\n",
        "    console.print(\"\\n[bold red]⚠️  Security Events:[/]\")\n",
        "    console.print(events_table)\n",
        "    console.print(event_logs_table)\n",
        "    console.print(\"\\n[bold green]📊 Traffic Patterns:[/]\")\n",
        "    console.print(traffic_table)\n",
        "\n",
        "# Define the STRESSED class as previously\n",
        "class STRESSED:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        log_type: str,\n",
        "        prompt_template_path: str,\n",
        "        token_max: int,\n",
        "        stressed_out: bool = False\n",
        "    ):\n",
        "        if token_max <= 0:\n",
        "            raise ValueError(\"token_max must be positive\")\n",
        "        if not os.path.exists(prompt_template_path):\n",
        "            raise FileNotFoundError(f\"Prompt template not found: {prompt_template_path}\")\n",
        "\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.log_type = log_type\n",
        "        self.token_max = token_max\n",
        "        self.stressed_out = stressed_out\n",
        "        # Load prompt template\n",
        "        with open(prompt_template_path, \"r\") as file:\n",
        "            self.prompt_template = file.read()\n",
        "\n",
        "        # Initialize generator\n",
        "        self.logger = outlines.generate.json(\n",
        "            self.model,\n",
        "            LogAnalysis,\n",
        "            sampler=outlines.samplers.greedy(),\n",
        "        )\n",
        "\n",
        "    def _to_prompt(self, text: str, pydantic_class: BaseModel) -> str:\n",
        "        if self.stressed_out:\n",
        "            stress_prompt = \"\"\"\n",
        "            You are a computer security intern that's really stressed out.\n",
        "            Your job is hard and you're not sure you're doing it well.\n",
        "\n",
        "            Your observations and summaries should reflect your anxiety.\n",
        "            Convey a sense of urgency and panic, be apologetic, and\n",
        "            generally act like you're not sure you can do your job.\n",
        "\n",
        "            In your summary, address your boss as \"boss\" and apologize for\n",
        "            any mistakes you've made even if you haven't made any.\n",
        "\n",
        "            Use \"um\" and \"ah\" a lot.\n",
        "            \"\"\"\n",
        "        else:\n",
        "            stress_prompt = \"\"\n",
        "\n",
        "        messages = []\n",
        "        if self.stressed_out:\n",
        "            messages.append({\"role\": \"system\", \"content\": stress_prompt})\n",
        "\n",
        "        messages.append(\n",
        "            {\"role\": \"user\", \"content\": self.prompt_template.format(\n",
        "                log_type=self.log_type,\n",
        "                logs=text,\n",
        "                model_schema=pydantic_class.model_json_schema(),\n",
        "                stress_prompt=stress_prompt,\n",
        "            )}\n",
        "        )\n",
        "\n",
        "        return self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "\n",
        "    def analyze_logs(\n",
        "    self,\n",
        "    logs: list[str],\n",
        "    chunk_size: int = 10,\n",
        "    format_output: bool = True\n",
        ") -> list[LogAnalysis]:\n",
        "        results = []\n",
        "        for i in range(0, len(logs), chunk_size):\n",
        "            chunked_logs = [log for log in logs[i:i+chunk_size] if log]\n",
        "            if not chunked_logs:\n",
        "                continue\n",
        "\n",
        "            log_ids = [f\"LOGID-{chr(65 + (j // 26) % 26)}{chr(65 + j % 26)}\"\n",
        "                      for j in range(len(chunked_logs))]\n",
        "            logs_with_ids = [f\"{log_id} {log}\" for log_id, log in zip(log_ids, chunked_logs)]\n",
        "            chunk = \"\\n\".join(logs_with_ids)\n",
        "            prompt = self._to_prompt(chunk, LogAnalysis)\n",
        "\n",
        "            try:\n",
        "                analysis = self.logger(prompt, max_tokens=self.token_max)\n",
        "                if format_output:\n",
        "                    format_log_analysis(analysis, logs_with_ids)\n",
        "                results.append(analysis)\n",
        "            except Exception as e:\n",
        "                print(f\"[Error] Skipping chunk {i // chunk_size + 1} due to error: {e}\")\n",
        "                continue\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# Streamlit UI for log analysis\n",
        "def run_log_analysis_app():\n",
        "    st.title(\"Log File Analysis with Security Prompt\")\n",
        "\n",
        "    # Upload log file\n",
        "    log_file = st.file_uploader(\"Upload Log File\", type=[\"txt\", \"log\"])\n",
        "    if log_file:\n",
        "        log_content = log_file.read().decode(\"utf-8\").splitlines()\n",
        "        st.write(f\"Uploaded Log File: {log_file.name}\")\n",
        "        st.text_area(\"Log Content\", \"\\n\".join(log_content), height=200)\n",
        "\n",
        "    # Upload the security prompt template file\n",
        "    prompt_file = st.file_uploader(\"Upload Security Prompt Template\", type=[\"txt\"])\n",
        "    if prompt_file:\n",
        "        prompt_content = prompt_file.read().decode(\"utf-8\")\n",
        "        st.write(f\"Uploaded Prompt Template File: {prompt_file.name}\")\n",
        "\n",
        "    # Choose whether to stress the analysis\n",
        "    stressed_out = st.checkbox(\"Stressed Out Intern Mode\", value=True)\n",
        "\n",
        "    # Configuration\n",
        "    token_max = st.slider(\"Max Tokens for Generation\", min_value=500, max_value=32000, value=32000, step=500)\n",
        "    model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "    log_type = \"web server\"\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "    if st.button(\"Analyze Logs\"):\n",
        "        # Load the model and tokenizer\n",
        "        model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "\n",
        "        model = outlines.models.vllm(\n",
        "            # The model we're using\n",
        "            model_name,\n",
        "\n",
        "            # The dtype to use for the model. bfloat16 is faster\n",
        "            # than the native float size.\n",
        "            dtype=torch.bfloat16,\n",
        "\n",
        "            # Enable prefix caching for faster inference\n",
        "            enable_prefix_caching=True,\n",
        "\n",
        "            # Disable sliding window -- this is required\n",
        "            # for prefix caching to work.\n",
        "            disable_sliding_window=True,\n",
        "\n",
        "            # The maximum sequence length for the model.\n",
        "            # Modify this if you have more memory available,\n",
        "            # and/or if your logs are longer.\n",
        "            max_model_len=32000,\n",
        "\n",
        "            max_num_seqs=10\n",
        "        )\n",
        "        # model = outlines.models.vllm(\n",
        "        #     model_name,\n",
        "        #     dtype=torch.bfloat16,\n",
        "        #     enable_prefix_caching=True,\n",
        "        #     disable_sliding_window=True,\n",
        "        #     max_model_len=token_max,\n",
        "        # )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # Initialize the STRESSED parser\n",
        "        parser = STRESSED(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            log_type=log_type,\n",
        "            prompt_template_path=\"/content/security-prompt.txt\",\n",
        "            token_max=token_max,\n",
        "            stressed_out=stressed_out\n",
        "        )\n",
        "\n",
        "        # Run the analysis\n",
        "        results = parser.analyze_logs(\n",
        "            log_content[:25],\n",
        "            chunk_size=5,\n",
        "            format_output=False\n",
        "\n",
        "        )\n",
        "        import json\n",
        "\n",
        "        # Convert successful results to serializable dicts\n",
        "        results_as_dicts = [r.dict() for r in results]\n",
        "\n",
        "        # Save to a JSON file\n",
        "        with open(\"analysis_results.json\", \"w\") as f:\n",
        "            json.dump(results_as_dicts, f, indent=2)\n",
        "\n",
        "        st.success(\"Saved partial results to 'analysis_results.json'\")\n",
        "\n",
        "        # Display results\n",
        "        for analysis in results[:5]:\n",
        "            st.subheader(\"Log Analysis Summary\")\n",
        "            st.write(analysis.summary)\n",
        "            st.subheader(\"Observations\")\n",
        "            st.write(\"\\n\".join(analysis.observations))\n",
        "            st.subheader(\"Security Events\")\n",
        "            for event in analysis.events:\n",
        "                st.write(f\"**Event Type:** {event.event_type}\")\n",
        "                st.write(f\"**Severity:** {event.severity.value}\")\n",
        "                st.write(f\"**Confidence Score:** {event.confidence_score * 100}%\")\n",
        "                st.write(f\"**Source IPs:** {', '.join([ip.ip_address for ip in event.source_ips])}\")\n",
        "                st.write(f\"**URL Pattern:** {event.url_pattern}\")\n",
        "                st.write(f\"**Possible Attacks:** {', '.join([attack.value for attack in event.possible_attack_patterns])}\")\n",
        "                st.write(f\"**Recommended Actions:** {', '.join(event.recommended_actions)}\")\n",
        "                st.write(\"---\")\n",
        "\n",
        "\n",
        "\n",
        "        st.title(\"📊 Log Analysis Viewer\")\n",
        "\n",
        "        json_path = \"/content/analysis_results.json\"\n",
        "\n",
        "        if os.path.exists(json_path):\n",
        "            with open(json_path, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            for idx, entry in enumerate(data[:5]):\n",
        "                st.header(f\"🔍 Analysis {idx + 1}\")\n",
        "                st.subheader(\"📝 Summary\")\n",
        "                st.write(entry[\"summary\"])\n",
        "\n",
        "                st.subheader(\"📌 Observations\")\n",
        "                for obs in entry.get(\"observations\", []):\n",
        "                    st.markdown(f\"- {obs}\")\n",
        "\n",
        "                st.subheader(\"🧠 Planning\")\n",
        "                for plan in entry.get(\"planning\", []):\n",
        "                    st.markdown(f\"- {plan}\")\n",
        "\n",
        "                st.subheader(\"⚠️ Events\")\n",
        "                if entry.get(\"events\"):\n",
        "                    for event in entry[\"events\"]:\n",
        "                        st.markdown(f\"**Event Type:** {event['event_type']}\")\n",
        "                        st.markdown(f\"**Severity:** {event['severity']}\")\n",
        "                        st.markdown(f\"**Requires Review:** {event['requires_human_review']}\")\n",
        "                        st.markdown(f\"**Confidence:** {event['confidence_score']}\")\n",
        "                        st.markdown(f\"**Source IPs:** {', '.join([ip['ip_address'] for ip in event['source_ips']])}\")\n",
        "                        st.markdown(f\"**Recommended Actions:**\")\n",
        "                        for action in event[\"recommended_actions\"]:\n",
        "                            st.markdown(f\"- {action}\")\n",
        "                        st.markdown(\"---\")\n",
        "                else:\n",
        "                    st.info(\"No security events found in this analysis.\")\n",
        "\n",
        "                st.subheader(\"🚦 Highest Severity\")\n",
        "                st.write(entry.get(\"highest_severity\", \"N/A\"))\n",
        "\n",
        "                st.subheader(\"⛑️ Immediate Attention Required\")\n",
        "                st.write(\"✅ Yes\" if entry.get(\"requires_immediate_attention\") else \"❌ No\")\n",
        "\n",
        "                st.markdown(\"---\")\n",
        "        else:\n",
        "            st.error(f\"File not found: {json_path}\")\n",
        "\n",
        "        st.title(\"📈 Log Analysis Visual Dashboard\")\n",
        "\n",
        "        json_path = \"/content/analysis_results.json\"\n",
        "\n",
        "        if os.path.exists(json_path):\n",
        "            with open(json_path, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            event_rows = []\n",
        "            for entry in data:\n",
        "                for event in entry.get(\"events\", []):\n",
        "                    event_rows.append({\n",
        "                        \"event_type\": event[\"event_type\"],\n",
        "                        \"severity\": event[\"severity\"],\n",
        "                        \"confidence_score\": event[\"confidence_score\"],\n",
        "                        \"source_ips\": \", \".join([ip[\"ip_address\"] for ip in event[\"source_ips\"]]),\n",
        "                    })\n",
        "\n",
        "            if event_rows:\n",
        "                df = pd.DataFrame(event_rows)\n",
        "\n",
        "                st.subheader(\"🚨 Event Type Distribution\")\n",
        "                # Normalize event types\n",
        "                df[\"event_type\"] = df[\"event_type\"].replace({\n",
        "                    \"BRUTE_FORCE\": \"Brute Force Attack\",\n",
        "                    \"BRUTE FORCE\": \"Brute Force Attack\",\n",
        "                    \"Brute Force\": \"Brute Force Attack\"\n",
        "                })\n",
        "\n",
        "                event_counts = df[\"event_type\"].value_counts()\n",
        "                fig1, ax1 = plt.subplots()\n",
        "                ax1.bar(event_counts.index, event_counts.values, color='skyblue')\n",
        "                ax1.set_title(\"Event Types\")\n",
        "                st.pyplot(fig1)\n",
        "\n",
        "                st.subheader(\"🔥 Severity Level Distribution\")\n",
        "                severity_counts = df[\"severity\"].value_counts()\n",
        "                fig2, ax2 = plt.subplots()\n",
        "                ax2.bar(severity_counts.index, severity_counts.values, color='salmon')\n",
        "                ax2.set_title(\"Severity Levels\")\n",
        "                st.pyplot(fig2)\n",
        "\n",
        "                st.subheader(\"🎯 Confidence Score Distribution\")\n",
        "                fig3, ax3 = plt.subplots()\n",
        "                ax3.hist(df[\"confidence_score\"], bins=10, color='lightgreen')\n",
        "                ax3.set_title(\"Confidence Scores\")\n",
        "                st.pyplot(fig3)\n",
        "\n",
        "                st.subheader(\"🌍 Source IP Address Frequency\")\n",
        "                ip_counts = Counter()\n",
        "                for ip_list in df[\"source_ips\"]:\n",
        "                    for ip in ip_list.split(\", \"):\n",
        "                        ip_counts[ip] += 1\n",
        "                ip_df = pd.DataFrame(ip_counts.items(), columns=[\"IP\", \"Count\"])\n",
        "                ip_df = ip_df.sort_values(by=\"Count\", ascending=False)\n",
        "                fig4, ax4 = plt.subplots()\n",
        "                ax4.bar(ip_df[\"IP\"], ip_df[\"Count\"], color='orange')\n",
        "                ax4.set_title(\"Top Source IPs\")\n",
        "                plt.xticks(rotation=45)\n",
        "                st.pyplot(fig4)\n",
        "\n",
        "            else:\n",
        "                st.warning(\"No events found in the file.\")\n",
        "        else:\n",
        "            st.error(f\"File not found: {json_path}\")\n",
        "\n",
        "\n",
        "        from reportlab.lib.pagesizes import A4\n",
        "        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\n",
        "        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "        from reportlab.lib.enums import TA_LEFT\n",
        "        from reportlab.lib import colors\n",
        "\n",
        "        from io import BytesIO\n",
        "\n",
        "        st.title(\"📄 Export Log Analysis to PDF\")\n",
        "\n",
        "        # JSON path\n",
        "        json_path = \"/content/analysis_results.json\"\n",
        "\n",
        "        if os.path.exists(json_path):\n",
        "            with open(json_path, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            buffer = BytesIO()\n",
        "            doc = SimpleDocTemplate(buffer, pagesize=A4)\n",
        "            styles = getSampleStyleSheet()\n",
        "            styles.add(ParagraphStyle(name=\"Heading\", fontSize=14, leading=16, spaceAfter=12, textColor=colors.darkblue))\n",
        "            styles.add(ParagraphStyle(name=\"SubHeading\", fontSize=12, leading=14, spaceAfter=6, textColor=colors.darkred))\n",
        "            styles.add(ParagraphStyle(name=\"Body\", fontSize=10, leading=12, spaceAfter=4, alignment=TA_LEFT))\n",
        "\n",
        "            elements = []\n",
        "\n",
        "            for idx, entry in enumerate(data):\n",
        "                elements.append(Paragraph(f\"🔍 Analysis {idx + 1}\", styles[\"Heading\"]))\n",
        "                elements.append(Paragraph(\"📝 Summary\", styles[\"SubHeading\"]))\n",
        "                elements.append(Paragraph(entry[\"summary\"], styles[\"Body\"]))\n",
        "\n",
        "                elements.append(Paragraph(\"📌 Observations\", styles[\"SubHeading\"]))\n",
        "                for obs in entry.get(\"observations\", []):\n",
        "                    elements.append(Paragraph(f\"• {obs}\", styles[\"Body\"]))\n",
        "\n",
        "                elements.append(Paragraph(\"🧠 Planning\", styles[\"SubHeading\"]))\n",
        "                for plan in entry.get(\"planning\", []):\n",
        "                    elements.append(Paragraph(f\"• {plan}\", styles[\"Body\"]))\n",
        "\n",
        "                elements.append(Paragraph(\"⚠️ Events\", styles[\"SubHeading\"]))\n",
        "                if entry.get(\"events\"):\n",
        "                    for event in entry[\"events\"]:\n",
        "                        elements.append(Paragraph(f\"Event Type: {event['event_type']}\", styles[\"Body\"]))\n",
        "                        elements.append(Paragraph(f\"Severity: {event['severity']}\", styles[\"Body\"]))\n",
        "                        elements.append(Paragraph(f\"Requires Review: {event['requires_human_review']}\", styles[\"Body\"]))\n",
        "                        elements.append(Paragraph(f\"Confidence: {event['confidence_score']}\", styles[\"Body\"]))\n",
        "                        elements.append(Paragraph(f\"Source IPs: {', '.join([ip['ip_address'] for ip in event['source_ips']])}\", styles[\"Body\"]))\n",
        "                        elements.append(Paragraph(\"Recommended Actions:\", styles[\"Body\"]))\n",
        "                        for action in event[\"recommended_actions\"]:\n",
        "                            elements.append(Paragraph(f\"• {action}\", styles[\"Body\"]))\n",
        "                        elements.append(Spacer(1, 6))\n",
        "                else:\n",
        "                    elements.append(Paragraph(\"No security events found in this analysis.\", styles[\"Body\"]))\n",
        "\n",
        "                elements.append(Paragraph(f\"🚦 Highest Severity: {entry.get('highest_severity', 'N/A')}\", styles[\"Body\"]))\n",
        "                attention = \"✅ Yes\" if entry.get(\"requires_immediate_attention\") else \"❌ No\"\n",
        "                elements.append(Paragraph(f\"⛑️ Immediate Attention Required: {attention}\", styles[\"Body\"]))\n",
        "                elements.append(PageBreak())\n",
        "\n",
        "            doc.build(elements)\n",
        "\n",
        "            st.success(\"PDF report created successfully!\")\n",
        "            st.download_button(\n",
        "                label=\"📥 Download PDF\",\n",
        "                data=buffer.getvalue(),\n",
        "                file_name=\"log_analysis_report.pdf\",\n",
        "                mime=\"application/pdf\"\n",
        "            )\n",
        "        else:\n",
        "            st.error(f\"File not found: {json_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_log_analysis_app()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt-sxXMRCOjJ",
        "outputId": "197573e2-b00b-4893-f650-a2d7111834c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.143.144.190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py --server.runOnSave false & npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLF7TQ9FCTEa",
        "outputId": "257ead94-14a7-442f-f6fd-8035b81b3b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.143.144.190:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0Kyour url is: https://yellow-worms-grab.loca.lt\n",
            "2025-04-29 14:22:04.986 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "2025-04-29 14:23:06.660500: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-29 14:23:06.681159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745936586.707244   12998 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745936586.714943   12998 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-29 14:23:06.741072: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "INFO 04-29 14:23:09 __init__.py:183] Automatically detected platform cuda.\n",
            "INFO 04-29 14:23:24 config.py:526] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
            "INFO 04-29 14:23:24 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='Qwen/Qwen2.5-Coder-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Coder-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-Coder-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[16,8,4,2,1],\"max_capture_size\":16}, use_cached_outputs=False, \n",
            "INFO 04-29 14:23:26 cuda.py:235] Using Flash Attention backend.\n",
            "[W429 14:23:27.608215055 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
            "INFO 04-29 14:23:27 model_runner.py:1111] Starting to load model Qwen/Qwen2.5-Coder-7B-Instruct...\n",
            "INFO 04-29 14:23:28 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards: 100% 4/4 [00:04<00:00,  1.04s/it]\n",
            "INFO 04-29 14:23:33 model_runner.py:1116] Loading model weights took 14.1931 GB\n",
            "INFO 04-29 14:23:36 worker.py:266] Memory profiling takes 2.53 seconds\n",
            "INFO 04-29 14:23:36 worker.py:266] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.90) = 35.60GiB\n",
            "INFO 04-29 14:23:36 worker.py:266] model weights take 14.19GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 4.25GiB; the rest of the memory reserved for KV Cache is 17.07GiB.\n",
            "INFO 04-29 14:23:36 executor_base.py:108] # CUDA blocks: 19972, # CPU blocks: 4681\n",
            "INFO 04-29 14:23:36 executor_base.py:113] Maximum concurrency for 32000 tokens per request: 9.99x\n",
            "INFO 04-29 14:23:39 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "Capturing CUDA graph shapes: 100% 5/5 [00:04<00:00,  1.21it/s]\n",
            "INFO 04-29 14:23:44 model_runner.py:1563] Graph capturing finished in 4 secs, took 0.08 GiB\n",
            "INFO 04-29 14:23:44 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 10.70 seconds\n",
            "Processed prompts: 100% 1/1 [00:08<00:00,  8.03s/it, est. speed input: 268.08 toks/s, output: 46.19 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:07<00:00,  7.73s/it, est. speed input: 286.50 toks/s, output: 43.44 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:06<00:00,  6.79s/it, est. speed input: 314.89 toks/s, output: 45.64 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:02<00:00,  2.92s/it, est. speed input: 697.69 toks/s, output: 40.42 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:08<00:00,  8.69s/it, est. speed input: 238.20 toks/s, output: 43.25 toks/s]\n",
            "/content/app.py:426: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  results_as_dicts = [r.dict() for r in results]\n",
            "2025-04-29 14:24:49.690 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "2025-04-29 14:42:58.666 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "INFO 04-29 14:49:45 config.py:526] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
            "INFO 04-29 14:49:45 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='Qwen/Qwen2.5-Coder-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Coder-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-Coder-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[16,8,4,2,1],\"max_capture_size\":16}, use_cached_outputs=False, \n",
            "INFO 04-29 14:49:46 model_runner.py:1111] Starting to load model Qwen/Qwen2.5-Coder-7B-Instruct...\n",
            "INFO 04-29 14:49:46 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n",
            "INFO 04-29 14:49:52 model_runner.py:1116] Loading model weights took 14.1852 GB\n",
            "INFO 04-29 14:49:54 worker.py:266] Memory profiling takes 2.17 seconds\n",
            "INFO 04-29 14:49:54 worker.py:266] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.90) = 35.60GiB\n",
            "INFO 04-29 14:49:54 worker.py:266] model weights take 14.19GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 4.24GiB; the rest of the memory reserved for KV Cache is 17.17GiB.\n",
            "INFO 04-29 14:49:55 executor_base.py:108] # CUDA blocks: 20098, # CPU blocks: 4681\n",
            "INFO 04-29 14:49:55 executor_base.py:113] Maximum concurrency for 32000 tokens per request: 10.05x\n",
            "INFO 04-29 14:49:55 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "Capturing CUDA graph shapes: 100% 5/5 [00:04<00:00,  1.17it/s]\n",
            "INFO 04-29 14:49:59 model_runner.py:1563] Graph capturing finished in 4 secs, took 0.04 GiB\n",
            "INFO 04-29 14:49:59 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 7.42 seconds\n",
            "Processed prompts: 100% 1/1 [00:08<00:00,  8.10s/it, est. speed input: 265.94 toks/s, output: 45.83 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:07<00:00,  7.72s/it, est. speed input: 287.18 toks/s, output: 43.54 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:06<00:00,  6.72s/it, est. speed input: 318.51 toks/s, output: 46.16 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:02<00:00,  2.92s/it, est. speed input: 698.66 toks/s, output: 40.47 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:08<00:00,  8.68s/it, est. speed input: 238.55 toks/s, output: 43.31 toks/s]\n",
            "/content/app.py:426: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  results_as_dicts = [r.dict() for r in results]\n",
            "INFO 04-29 15:10:10 config.py:526] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
            "INFO 04-29 15:10:10 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='Qwen/Qwen2.5-Coder-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Coder-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-Coder-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[16,8,4,2,1],\"max_capture_size\":16}, use_cached_outputs=False, \n",
            "INFO 04-29 15:10:12 model_runner.py:1111] Starting to load model Qwen/Qwen2.5-Coder-7B-Instruct...\n",
            "INFO 04-29 15:10:12 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards: 100% 4/4 [00:04<00:00,  1.05s/it]\n",
            "INFO 04-29 15:10:18 model_runner.py:1116] Loading model weights took 14.1852 GB\n",
            "INFO 04-29 15:10:20 worker.py:266] Memory profiling takes 2.22 seconds\n",
            "INFO 04-29 15:10:20 worker.py:266] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.90) = 35.60GiB\n",
            "INFO 04-29 15:10:20 worker.py:266] model weights take 14.19GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 4.24GiB; the rest of the memory reserved for KV Cache is 17.17GiB.\n",
            "INFO 04-29 15:10:21 executor_base.py:108] # CUDA blocks: 20098, # CPU blocks: 4681\n",
            "INFO 04-29 15:10:21 executor_base.py:113] Maximum concurrency for 32000 tokens per request: 10.05x\n",
            "INFO 04-29 15:10:21 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "Capturing CUDA graph shapes: 100% 5/5 [00:04<00:00,  1.11it/s]\n",
            "INFO 04-29 15:10:26 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.04 GiB\n",
            "INFO 04-29 15:10:26 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 7.74 seconds\n",
            "Processed prompts: 100% 1/1 [00:08<00:00,  8.10s/it, est. speed input: 265.79 toks/s, output: 45.80 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:07<00:00,  7.79s/it, est. speed input: 284.56 toks/s, output: 43.15 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:06<00:00,  6.81s/it, est. speed input: 313.98 toks/s, output: 45.50 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:02<00:00,  3.00s/it, est. speed input: 679.46 toks/s, output: 39.36 toks/s]\n",
            "Processed prompts: 100% 1/1 [00:08<00:00,  8.87s/it, est. speed input: 233.56 toks/s, output: 42.40 toks/s]\n",
            "/content/app.py:426: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  results_as_dicts = [r.dict() for r in results]\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "[rank0]:[W429 15:28:20.397550903 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r8C8vItDNnIU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}